2025-09-28 13:11:02,236 - TrainingLogger - VERBOSE - 
 Initializing positions for replay buffer episode 0 with users and eavesdroppers positions:
   !~ Users Positions: [[124, 40], [138.5, 81]] 
   !~ Eavesdroppers Positions: [[132, 50], [128, 85]] 

2025-09-28 13:11:18,656 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 500
     |--> Training the NN takes 0.0261 sec on average across 405 steps 
  |--> LOCAL AVERAGE REWARD 1.96484375, MAX INSTANT REWARD REACHED 4.616737755675749
  |--> LOCAL AVERAGE BASIC REWARD 1.96484375, MAXIMUM INSTANT BASIC REWARD: 4.6167
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(4.617), 'Downlink reward': np.float16(1.408), 'Uplink reward': np.float16(3.844), 'Eavesdropping reward': np.float16(0.636), 'Eavesdropping_Downlink_reward': np.float16(0.3994), 'Eavesdropping_Uplink_reward': np.float16(0.526)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0519), 'Uplink reward': np.float16(0.03522), 'Eavesdropping reward': np.float16(0.7124), 'Eavesdropping_Downlink_reward': np.float16(0.1349), 'Eavesdropping_Uplink_reward': np.float16(0.5776)}}
 |--> ACTOR LOSS 3.070011854171753, CRITIC LOSS 0.06028507277369499
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5232
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:11:40,310 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 1000
     |--> Training the NN takes 0.0282 sec on average across 905 steps 
  |--> LOCAL AVERAGE REWARD 5.22265625, MAX INSTANT REWARD REACHED 7.749159433507629
  |--> LOCAL AVERAGE BASIC REWARD 5.22265625, MAXIMUM INSTANT BASIC REWARD: 7.7492
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(7.75), 'Downlink reward': np.float16(3.307), 'Uplink reward': np.float16(5.47), 'Eavesdropping reward': np.float16(1.025), 'Eavesdropping_Downlink_reward': np.float16(0.787), 'Eavesdropping_Uplink_reward': np.float16(0.2386)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.006294), 'Uplink reward': np.float16(0.01209), 'Eavesdropping reward': np.float16(0.53), 'Eavesdropping_Downlink_reward': np.float16(0.02507), 'Eavesdropping_Uplink_reward': np.float16(0.505)}}
 |--> ACTOR LOSS 4.879757881164551, CRITIC LOSS 0.033762358129024506
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:12:04,016 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 1500
     |--> Training the NN takes 0.0304 sec on average across 1405 steps 
  |--> LOCAL AVERAGE REWARD 6.9921875, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 6.9921875, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 5.68853759765625, CRITIC LOSS 0.07185961306095123
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:12:27,346 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 2000
     |--> Training the NN takes 0.0315 sec on average across 1905 steps 
  |--> LOCAL AVERAGE REWARD 7.5625, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 7.5625, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 6.702704906463623, CRITIC LOSS 0.0854339599609375
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:12:50,948 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 2500
     |--> Training the NN takes 0.0322 sec on average across 2405 steps 
  |--> LOCAL AVERAGE REWARD 7.421875, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 7.421875, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 6.975600719451904, CRITIC LOSS 0.12715160846710205
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:13:14,461 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 3000
     |--> Training the NN takes 0.0327 sec on average across 2905 steps 
  |--> LOCAL AVERAGE REWARD 7.296875, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 7.296875, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 7.200343132019043, CRITIC LOSS 0.18420033156871796
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:13:36,704 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 3500
     |--> Training the NN takes 0.0329 sec on average across 3405 steps 
  |--> LOCAL AVERAGE REWARD 6.3828125, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 6.3828125, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 7.102604866027832, CRITIC LOSS 0.34265780448913574
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:13:59,649 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 4000
     |--> Training the NN takes 0.0331 sec on average across 3905 steps 
  |--> LOCAL AVERAGE REWARD 6.91796875, MAX INSTANT REWARD REACHED 10.121661909170163
  |--> LOCAL AVERAGE BASIC REWARD 6.91796875, MAXIMUM INSTANT BASIC REWARD: 10.1217
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.125), 'Downlink reward': np.float16(4.918), 'Uplink reward': np.float16(6.008), 'Eavesdropping reward': np.float16(0.803), 'Eavesdropping_Downlink_reward': np.float16(0.6577), 'Eavesdropping_Uplink_reward': np.float16(0.1456)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000387), 'Uplink reward': np.float16(0.003202), 'Eavesdropping reward': np.float16(1.115), 'Eavesdropping_Downlink_reward': np.float16(0.00205), 'Eavesdropping_Uplink_reward': np.float16(1.114)}}
 |--> ACTOR LOSS 7.838029384613037, CRITIC LOSS 0.15321579575538635
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:14:23,277 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 4500
     |--> Training the NN takes 0.0334 sec on average across 4405 steps 
  |--> LOCAL AVERAGE REWARD 7.46484375, MAX INSTANT REWARD REACHED 10.255364907415414
  |--> LOCAL AVERAGE BASIC REWARD 7.46484375, MAXIMUM INSTANT BASIC REWARD: 10.2554
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.26), 'Downlink reward': np.float16(4.965), 'Uplink reward': np.float16(6.168), 'Eavesdropping reward': np.float16(0.8774), 'Eavesdropping_Downlink_reward': np.float16(0.8755), 'Eavesdropping_Uplink_reward': np.float16(0.0369)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000553), 'Uplink reward': np.float16(0.003082), 'Eavesdropping reward': np.float16(0.775), 'Eavesdropping_Downlink_reward': np.float16(0.003487), 'Eavesdropping_Uplink_reward': np.float16(0.773)}}
 |--> ACTOR LOSS 7.674151420593262, CRITIC LOSS 0.08827897906303406
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:14:46,272 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 5000
     |--> Training the NN takes 0.0336 sec on average across 4905 steps 
  |--> LOCAL AVERAGE REWARD 7.28515625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.28515625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.59681510925293, CRITIC LOSS 0.10747798532247543
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:15:09,439 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 5500
     |--> Training the NN takes 0.0338 sec on average across 5405 steps 
  |--> LOCAL AVERAGE REWARD 7.3984375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.3984375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.131555080413818, CRITIC LOSS 0.1585521250963211
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:15:27,841 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 6000
     |--> Training the NN takes 0.0332 sec on average across 5905 steps 
  |--> LOCAL AVERAGE REWARD 6.07421875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.07421875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.113179206848145, CRITIC LOSS 0.12748250365257263
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:15:51,270 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 6500
     |--> Training the NN takes 0.0334 sec on average across 6405 steps 
  |--> LOCAL AVERAGE REWARD 6.15234375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.15234375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.712852478027344, CRITIC LOSS 0.06783008575439453
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:16:14,411 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 7000
     |--> Training the NN takes 0.0335 sec on average across 6905 steps 
  |--> LOCAL AVERAGE REWARD 6.1796875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.1796875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.808501243591309, CRITIC LOSS 0.03741275519132614
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:16:36,969 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 7500
     |--> Training the NN takes 0.0336 sec on average across 7405 steps 
  |--> LOCAL AVERAGE REWARD 6.37109375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.37109375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.902098655700684, CRITIC LOSS 0.05039745569229126
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:16:59,599 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 8000
     |--> Training the NN takes 0.0336 sec on average across 7905 steps 
  |--> LOCAL AVERAGE REWARD 6.20703125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.20703125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.989070415496826, CRITIC LOSS 0.029512135311961174
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:17:22,780 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 8500
     |--> Training the NN takes 0.0337 sec on average across 8405 steps 
  |--> LOCAL AVERAGE REWARD 5.703125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 5.703125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.563771724700928, CRITIC LOSS 0.025504525750875473
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:17:46,020 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 9000
     |--> Training the NN takes 0.0338 sec on average across 8905 steps 
  |--> LOCAL AVERAGE REWARD 3.47265625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 3.47265625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.450393199920654, CRITIC LOSS 0.023431580513715744
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:18:08,692 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 9500
     |--> Training the NN takes 0.0339 sec on average across 9405 steps 
  |--> LOCAL AVERAGE REWARD 1.9248046875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 1.9248046875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.406072616577148, CRITIC LOSS 0.016423989087343216
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5227
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:18:31,647 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 10000
     |--> Training the NN takes 0.0339 sec on average across 9905 steps 
  |--> LOCAL AVERAGE REWARD 4.41015625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 4.41015625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.192447662353516, CRITIC LOSS 0.014831244945526123
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:18:54,558 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 10500
     |--> Training the NN takes 0.0340 sec on average across 10405 steps 
  |--> LOCAL AVERAGE REWARD 5.91796875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 5.91796875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.18797779083252, CRITIC LOSS 0.004773159511387348
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:19:17,735 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 11000
     |--> Training the NN takes 0.0340 sec on average across 10905 steps 
  |--> LOCAL AVERAGE REWARD 6.5703125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.5703125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.652979850769043, CRITIC LOSS 0.004856075160205364
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:19:40,666 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 11500
     |--> Training the NN takes 0.0341 sec on average across 11405 steps 
  |--> LOCAL AVERAGE REWARD 6.6953125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.6953125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.520350456237793, CRITIC LOSS 0.007052630186080933
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:20:02,340 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 12000
     |--> Training the NN takes 0.0340 sec on average across 11905 steps 
  |--> LOCAL AVERAGE REWARD 6.75390625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.75390625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.169572353363037, CRITIC LOSS 0.006018197163939476
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:20:24,205 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 12500
     |--> Training the NN takes 0.0340 sec on average across 12405 steps 
  |--> LOCAL AVERAGE REWARD 6.91796875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.91796875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.009754180908203, CRITIC LOSS 0.0021068621426820755
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:20:46,283 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 13000
     |--> Training the NN takes 0.0340 sec on average across 12905 steps 
  |--> LOCAL AVERAGE REWARD 6.9453125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.9453125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 6.890907287597656, CRITIC LOSS 0.0026464127004146576
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:21:08,175 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 13500
     |--> Training the NN takes 0.0340 sec on average across 13405 steps 
  |--> LOCAL AVERAGE REWARD 6.83203125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.83203125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 6.9308085441589355, CRITIC LOSS 0.0050695729441940784
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:21:29,591 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 14000
     |--> Training the NN takes 0.0339 sec on average across 13905 steps 
  |--> LOCAL AVERAGE REWARD 6.75, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.75, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 6.997707366943359, CRITIC LOSS 0.00689157098531723
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:21:52,106 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 14500
     |--> Training the NN takes 0.0339 sec on average across 14405 steps 
  |--> LOCAL AVERAGE REWARD 6.73828125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.73828125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.357705116271973, CRITIC LOSS 0.11998683214187622
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:22:14,064 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 15000
     |--> Training the NN takes 0.0339 sec on average across 14905 steps 
  |--> LOCAL AVERAGE REWARD 5.96875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 5.96875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.5066022872924805, CRITIC LOSS 0.06393411755561829
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:22:35,741 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 15500
     |--> Training the NN takes 0.0339 sec on average across 15405 steps 
  |--> LOCAL AVERAGE REWARD 6.3671875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.3671875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.577979564666748, CRITIC LOSS 0.07557521760463715
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:22:57,992 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 16000
     |--> Training the NN takes 0.0339 sec on average across 15905 steps 
  |--> LOCAL AVERAGE REWARD 6.69921875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.69921875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.5532636642456055, CRITIC LOSS 0.13119438290596008
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:23:20,182 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 16500
     |--> Training the NN takes 0.0339 sec on average across 16405 steps 
  |--> LOCAL AVERAGE REWARD 6.73828125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 6.73828125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.422041893005371, CRITIC LOSS 0.07689046859741211
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:23:41,954 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 17000
     |--> Training the NN takes 0.0338 sec on average across 16905 steps 
  |--> LOCAL AVERAGE REWARD 7.265625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.265625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.024676322937012, CRITIC LOSS 0.1890943944454193
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:24:03,792 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 17500
     |--> Training the NN takes 0.0338 sec on average across 17405 steps 
  |--> LOCAL AVERAGE REWARD 7.828125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.828125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 7.869457244873047, CRITIC LOSS 0.07628241181373596
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:24:26,353 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 18000
     |--> Training the NN takes 0.0338 sec on average across 17905 steps 
  |--> LOCAL AVERAGE REWARD 7.67578125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.67578125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.044245719909668, CRITIC LOSS 0.25208818912506104
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:24:48,422 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 18500
     |--> Training the NN takes 0.0338 sec on average across 18405 steps 
  |--> LOCAL AVERAGE REWARD 7.62109375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.62109375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.178289413452148, CRITIC LOSS 0.20998762547969818
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:25:10,854 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 19000
     |--> Training the NN takes 0.0338 sec on average across 18905 steps 
  |--> LOCAL AVERAGE REWARD 7.015625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.015625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.32750129699707, CRITIC LOSS 0.22225646674633026
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:25:33,202 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 19500
     |--> Training the NN takes 0.0338 sec on average across 19405 steps 
  |--> LOCAL AVERAGE REWARD 7.33203125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.33203125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.159491539001465, CRITIC LOSS 0.19384005665779114
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:25:55,191 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 20000
     |--> Training the NN takes 0.0338 sec on average across 19905 steps 
  |--> LOCAL AVERAGE REWARD 7.34765625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.34765625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.456535339355469, CRITIC LOSS 0.19502684473991394
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:26:17,484 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 20500
     |--> Training the NN takes 0.0338 sec on average across 20405 steps 
  |--> LOCAL AVERAGE REWARD 7.37890625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.37890625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.688432693481445, CRITIC LOSS 0.16823753714561462
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:26:39,482 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 21000
     |--> Training the NN takes 0.0338 sec on average across 20905 steps 
  |--> LOCAL AVERAGE REWARD 7.41796875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.41796875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.738685607910156, CRITIC LOSS 0.21724271774291992
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:27:01,749 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 21500
     |--> Training the NN takes 0.0338 sec on average across 21405 steps 
  |--> LOCAL AVERAGE REWARD 7.40234375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.40234375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.601314544677734, CRITIC LOSS 0.1799604594707489
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:27:25,347 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 22000
     |--> Training the NN takes 0.0339 sec on average across 21905 steps 
  |--> LOCAL AVERAGE REWARD 7.58203125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.58203125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.778029441833496, CRITIC LOSS 0.17443495988845825
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:27:48,204 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 22500
     |--> Training the NN takes 0.0339 sec on average across 22405 steps 
  |--> LOCAL AVERAGE REWARD 7.4375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.4375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.629627227783203, CRITIC LOSS 0.17381131649017334
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:28:10,243 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 23000
     |--> Training the NN takes 0.0339 sec on average across 22905 steps 
  |--> LOCAL AVERAGE REWARD 7.78125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.78125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.856908798217773, CRITIC LOSS 0.14742563664913177
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:28:31,864 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 23500
     |--> Training the NN takes 0.0338 sec on average across 23405 steps 
  |--> LOCAL AVERAGE REWARD 7.72265625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.72265625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 8.839338302612305, CRITIC LOSS 0.14108887314796448
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:28:53,602 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 24000
     |--> Training the NN takes 0.0338 sec on average across 23905 steps 
  |--> LOCAL AVERAGE REWARD 8.03125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.03125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.338995933532715, CRITIC LOSS 0.11635380983352661
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:29:15,768 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 24500
     |--> Training the NN takes 0.0338 sec on average across 24405 steps 
  |--> LOCAL AVERAGE REWARD 8.3359375, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.3359375, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.435222625732422, CRITIC LOSS 0.08212704211473465
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:29:37,614 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 25000
     |--> Training the NN takes 0.0338 sec on average across 24905 steps 
  |--> LOCAL AVERAGE REWARD 8.1953125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.1953125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.661540031433105, CRITIC LOSS 0.07836728543043137
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:29:59,712 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 25500
     |--> Training the NN takes 0.0338 sec on average across 25405 steps 
  |--> LOCAL AVERAGE REWARD 8.2265625, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.2265625, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.653449058532715, CRITIC LOSS 0.11635798215866089
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:30:21,907 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 26000
     |--> Training the NN takes 0.0338 sec on average across 25905 steps 
  |--> LOCAL AVERAGE REWARD 8.296875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.296875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.562294006347656, CRITIC LOSS 0.08345391601324081
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:30:43,195 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 26500
     |--> Training the NN takes 0.0338 sec on average across 26405 steps 
  |--> LOCAL AVERAGE REWARD 8.0078125, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 8.0078125, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.917559623718262, CRITIC LOSS 0.1376166194677353
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:31:03,835 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 27000
     |--> Training the NN takes 0.0337 sec on average across 26905 steps 
  |--> LOCAL AVERAGE REWARD 7.94921875, MAX INSTANT REWARD REACHED 10.647378614714597
  |--> LOCAL AVERAGE BASIC REWARD 7.94921875, MAXIMUM INSTANT BASIC REWARD: 10.6474
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.65), 'Downlink reward': np.float16(5.125), 'Uplink reward': np.float16(6.35), 'Eavesdropping reward': np.float16(0.831), 'Eavesdropping_Downlink_reward': np.float16(0.816), 'Eavesdropping_Uplink_reward': np.float16(0.02798)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0001328), 'Uplink reward': np.float16(0.0003853), 'Eavesdropping reward': np.float16(0.826), 'Eavesdropping_Downlink_reward': np.float16(0.00472), 'Eavesdropping_Uplink_reward': np.float16(0.823)}}
 |--> ACTOR LOSS 9.584062576293945, CRITIC LOSS 0.09490884840488434
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:31:24,376 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 27500
     |--> Training the NN takes 0.0337 sec on average across 27405 steps 
  |--> LOCAL AVERAGE REWARD 8.4296875, MAX INSTANT REWARD REACHED 10.654544102944076
  |--> LOCAL AVERAGE BASIC REWARD 8.4296875, MAXIMUM INSTANT BASIC REWARD: 10.6545
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.66), 'Downlink reward': np.float16(5.02), 'Uplink reward': np.float16(6.004), 'Eavesdropping reward': np.float16(0.3672), 'Eavesdropping_Downlink_reward': np.float16(0.3345), 'Eavesdropping_Uplink_reward': np.float16(0.03265)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000594), 'Uplink reward': np.float16(0.001548), 'Eavesdropping reward': np.float16(1.228), 'Eavesdropping_Downlink_reward': np.float16(0.002699), 'Eavesdropping_Uplink_reward': np.float16(1.226)}}
 |--> ACTOR LOSS 9.453710556030273, CRITIC LOSS 0.07075763493776321
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:31:43,857 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 28000
     |--> Training the NN takes 0.0336 sec on average across 27905 steps 
  |--> LOCAL AVERAGE REWARD 8.65625, MAX INSTANT REWARD REACHED 10.748357295728606
  |--> LOCAL AVERAGE BASIC REWARD 8.65625, MAXIMUM INSTANT BASIC REWARD: 10.7484
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.75), 'Downlink reward': np.float16(5.234), 'Uplink reward': np.float16(5.875), 'Eavesdropping reward': np.float16(0.3618), 'Eavesdropping_Downlink_reward': np.float16(0.3591), 'Eavesdropping_Uplink_reward': np.float16(0.007107)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.001053), 'Uplink reward': np.float16(0.00829), 'Eavesdropping reward': np.float16(2.324), 'Eavesdropping_Downlink_reward': np.float16(0.001171), 'Eavesdropping_Uplink_reward': np.float16(2.324)}}
 |--> ACTOR LOSS 9.785513877868652, CRITIC LOSS 0.11110399663448334
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:32:02,168 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 28500
     |--> Training the NN takes 0.0335 sec on average across 28405 steps 
  |--> LOCAL AVERAGE REWARD 8.7109375, MAX INSTANT REWARD REACHED 10.804799524124366
  |--> LOCAL AVERAGE BASIC REWARD 8.7109375, MAXIMUM INSTANT BASIC REWARD: 10.8048
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.805), 'Downlink reward': np.float16(4.805), 'Uplink reward': np.float16(6.246), 'Eavesdropping reward': np.float16(0.2494), 'Eavesdropping_Downlink_reward': np.float16(0.249), 'Eavesdropping_Uplink_reward': np.float16(0.04532)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(7.1e-05), 'Uplink reward': np.float16(0.001873), 'Eavesdropping reward': np.float16(1.829), 'Eavesdropping_Downlink_reward': np.float16(0.0003169), 'Eavesdropping_Uplink_reward': np.float16(1.829)}}
 |--> ACTOR LOSS 9.798933029174805, CRITIC LOSS 0.0716080367565155
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:32:19,965 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 29000
     |--> Training the NN takes 0.0334 sec on average across 28905 steps 
  |--> LOCAL AVERAGE REWARD 8.75, MAX INSTANT REWARD REACHED 10.804799524124366
  |--> LOCAL AVERAGE BASIC REWARD 8.75, MAXIMUM INSTANT BASIC REWARD: 10.8048
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.805), 'Downlink reward': np.float16(4.805), 'Uplink reward': np.float16(6.246), 'Eavesdropping reward': np.float16(0.2494), 'Eavesdropping_Downlink_reward': np.float16(0.249), 'Eavesdropping_Uplink_reward': np.float16(0.04532)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(7.1e-05), 'Uplink reward': np.float16(0.001873), 'Eavesdropping reward': np.float16(1.829), 'Eavesdropping_Downlink_reward': np.float16(0.0003169), 'Eavesdropping_Uplink_reward': np.float16(1.829)}}
 |--> ACTOR LOSS 10.15864372253418, CRITIC LOSS 0.14363789558410645
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:32:36,814 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 29500
     |--> Training the NN takes 0.0332 sec on average across 29405 steps 
  |--> LOCAL AVERAGE REWARD 8.7578125, MAX INSTANT REWARD REACHED 10.804799524124366
  |--> LOCAL AVERAGE BASIC REWARD 8.7578125, MAXIMUM INSTANT BASIC REWARD: 10.8048
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(10.805), 'Downlink reward': np.float16(4.805), 'Uplink reward': np.float16(6.246), 'Eavesdropping reward': np.float16(0.2494), 'Eavesdropping_Downlink_reward': np.float16(0.249), 'Eavesdropping_Uplink_reward': np.float16(0.04532)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(7.1e-05), 'Uplink reward': np.float16(0.001873), 'Eavesdropping reward': np.float16(1.829), 'Eavesdropping_Downlink_reward': np.float16(0.0003169), 'Eavesdropping_Uplink_reward': np.float16(1.829)}}
 |--> ACTOR LOSS 10.217363357543945, CRITIC LOSS 0.10141878575086594
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:32:52,166 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 30000
     |--> Training the NN takes 0.0330 sec on average across 29905 steps 
  |--> LOCAL AVERAGE REWARD 8.90625, MAX INSTANT REWARD REACHED 11.01058891733699
  |--> LOCAL AVERAGE BASIC REWARD 8.90625, MAXIMUM INSTANT BASIC REWARD: 11.0106
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.01), 'Downlink reward': np.float16(4.715), 'Uplink reward': np.float16(6.496), 'Eavesdropping reward': np.float16(0.2035), 'Eavesdropping_Downlink_reward': np.float16(0.1522), 'Eavesdropping_Uplink_reward': np.float16(0.09607)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(1.78e-05), 'Uplink reward': np.float16(0.0004375), 'Eavesdropping reward': np.float16(2.266), 'Eavesdropping_Downlink_reward': np.float16(0.0002227), 'Eavesdropping_Uplink_reward': np.float16(2.266)}}
 |--> ACTOR LOSS 9.442252159118652, CRITIC LOSS 0.06931549310684204
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:33:07,675 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 30500
     |--> Training the NN takes 0.0329 sec on average across 30405 steps 
  |--> LOCAL AVERAGE REWARD 8.90625, MAX INSTANT REWARD REACHED 11.313641599083793
  |--> LOCAL AVERAGE BASIC REWARD 8.90625, MAXIMUM INSTANT BASIC REWARD: 11.3136
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.31), 'Downlink reward': np.float16(5.04), 'Uplink reward': np.float16(6.46), 'Eavesdropping reward': np.float16(0.1875), 'Eavesdropping_Downlink_reward': np.float16(0.1748), 'Eavesdropping_Uplink_reward': np.float16(0.04034)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(6.217e-05), 'Uplink reward': np.float16(0.00028), 'Eavesdropping reward': np.float16(2.09), 'Eavesdropping_Downlink_reward': np.float16(0.000936), 'Eavesdropping_Uplink_reward': np.float16(2.09)}}
 |--> ACTOR LOSS 10.115145683288574, CRITIC LOSS 0.09387823194265366
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:33:20,995 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 31000
     |--> Training the NN takes 0.0326 sec on average across 30905 steps 
  |--> LOCAL AVERAGE REWARD 8.8671875, MAX INSTANT REWARD REACHED 11.313641599083793
  |--> LOCAL AVERAGE BASIC REWARD 8.8671875, MAXIMUM INSTANT BASIC REWARD: 11.3136
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.31), 'Downlink reward': np.float16(5.04), 'Uplink reward': np.float16(6.46), 'Eavesdropping reward': np.float16(0.1875), 'Eavesdropping_Downlink_reward': np.float16(0.1748), 'Eavesdropping_Uplink_reward': np.float16(0.04034)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(6.217e-05), 'Uplink reward': np.float16(0.00028), 'Eavesdropping reward': np.float16(2.09), 'Eavesdropping_Downlink_reward': np.float16(0.000936), 'Eavesdropping_Uplink_reward': np.float16(2.09)}}
 |--> ACTOR LOSS 9.945785522460938, CRITIC LOSS 0.07493754476308823
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:33:31,372 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 31500
     |--> Training the NN takes 0.0323 sec on average across 31405 steps 
  |--> LOCAL AVERAGE REWARD 8.484375, MAX INSTANT REWARD REACHED 11.42749344607449
  |--> LOCAL AVERAGE BASIC REWARD 8.484375, MAXIMUM INSTANT BASIC REWARD: 11.4275
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.43), 'Downlink reward': np.float16(5.19), 'Uplink reward': np.float16(6.48), 'Eavesdropping reward': np.float16(0.2457), 'Eavesdropping_Downlink_reward': np.float16(0.2251), 'Eavesdropping_Uplink_reward': np.float16(0.0936)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000138), 'Uplink reward': np.float16(0.00169), 'Eavesdropping reward': np.float16(1.672), 'Eavesdropping_Downlink_reward': np.float16(0.0004566), 'Eavesdropping_Uplink_reward': np.float16(1.672)}}
 |--> ACTOR LOSS 9.265192985534668, CRITIC LOSS 0.1787509173154831
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:33:41,611 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 32000
     |--> Training the NN takes 0.0320 sec on average across 31905 steps 
  |--> LOCAL AVERAGE REWARD 9.3046875, MAX INSTANT REWARD REACHED 11.42749344607449
  |--> LOCAL AVERAGE BASIC REWARD 9.3046875, MAXIMUM INSTANT BASIC REWARD: 11.4275
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.43), 'Downlink reward': np.float16(5.19), 'Uplink reward': np.float16(6.48), 'Eavesdropping reward': np.float16(0.2457), 'Eavesdropping_Downlink_reward': np.float16(0.2251), 'Eavesdropping_Uplink_reward': np.float16(0.0936)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000138), 'Uplink reward': np.float16(0.00169), 'Eavesdropping reward': np.float16(1.672), 'Eavesdropping_Downlink_reward': np.float16(0.0004566), 'Eavesdropping_Uplink_reward': np.float16(1.672)}}
 |--> ACTOR LOSS 10.512073516845703, CRITIC LOSS 0.09827318042516708
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:33:53,252 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 32500
     |--> Training the NN takes 0.0318 sec on average across 32405 steps 
  |--> LOCAL AVERAGE REWARD 9.8125, MAX INSTANT REWARD REACHED 11.712058433532032
  |--> LOCAL AVERAGE BASIC REWARD 9.8125, MAXIMUM INSTANT BASIC REWARD: 11.7121
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.71), 'Downlink reward': np.float16(5.54), 'Uplink reward': np.float16(6.367), 'Eavesdropping reward': np.float16(0.1968), 'Eavesdropping_Downlink_reward': np.float16(0.1818), 'Eavesdropping_Uplink_reward': np.float16(0.1383)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0002133), 'Uplink reward': np.float16(0.003716), 'Eavesdropping reward': np.float16(1.944), 'Eavesdropping_Downlink_reward': np.float16(0.0002254), 'Eavesdropping_Uplink_reward': np.float16(1.944)}}
 |--> ACTOR LOSS 10.710531234741211, CRITIC LOSS 0.10648123919963837
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:34:05,969 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 33000
     |--> Training the NN takes 0.0316 sec on average across 32905 steps 
  |--> LOCAL AVERAGE REWARD 9.40625, MAX INSTANT REWARD REACHED 11.754175965453328
  |--> LOCAL AVERAGE BASIC REWARD 9.40625, MAXIMUM INSTANT BASIC REWARD: 11.7542
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.76), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.426), 'Eavesdropping reward': np.float16(0.2983), 'Eavesdropping_Downlink_reward': np.float16(0.2908), 'Eavesdropping_Uplink_reward': np.float16(0.09454)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000257), 'Uplink reward': np.float16(0.002892), 'Eavesdropping reward': np.float16(1.757), 'Eavesdropping_Downlink_reward': np.float16(0.0004823), 'Eavesdropping_Uplink_reward': np.float16(1.757)}}
 |--> ACTOR LOSS 10.35374641418457, CRITIC LOSS 0.07982225716114044
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:34:18,434 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 33500
     |--> Training the NN takes 0.0314 sec on average across 33405 steps 
  |--> LOCAL AVERAGE REWARD 9.0859375, MAX INSTANT REWARD REACHED 11.754175965453328
  |--> LOCAL AVERAGE BASIC REWARD 9.0859375, MAXIMUM INSTANT BASIC REWARD: 11.7542
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.76), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.426), 'Eavesdropping reward': np.float16(0.2983), 'Eavesdropping_Downlink_reward': np.float16(0.2908), 'Eavesdropping_Uplink_reward': np.float16(0.09454)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000257), 'Uplink reward': np.float16(0.002892), 'Eavesdropping reward': np.float16(1.757), 'Eavesdropping_Downlink_reward': np.float16(0.0004823), 'Eavesdropping_Uplink_reward': np.float16(1.757)}}
 |--> ACTOR LOSS 10.600902557373047, CRITIC LOSS 0.10280457139015198
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:34:30,381 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 34000
     |--> Training the NN takes 0.0311 sec on average across 33905 steps 
  |--> LOCAL AVERAGE REWARD 9.296875, MAX INSTANT REWARD REACHED 11.754175965453328
  |--> LOCAL AVERAGE BASIC REWARD 9.296875, MAXIMUM INSTANT BASIC REWARD: 11.7542
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.76), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.426), 'Eavesdropping reward': np.float16(0.2983), 'Eavesdropping_Downlink_reward': np.float16(0.2908), 'Eavesdropping_Uplink_reward': np.float16(0.09454)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.000257), 'Uplink reward': np.float16(0.002892), 'Eavesdropping reward': np.float16(1.757), 'Eavesdropping_Downlink_reward': np.float16(0.0004823), 'Eavesdropping_Uplink_reward': np.float16(1.757)}}
 |--> ACTOR LOSS 10.148368835449219, CRITIC LOSS 0.10635468363761902
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:34:42,041 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 34500
     |--> Training the NN takes 0.0309 sec on average across 34405 steps 
  |--> LOCAL AVERAGE REWARD 9.1953125, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 9.1953125, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 9.852216720581055, CRITIC LOSS 0.07117209583520889
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:34:51,895 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 35000
     |--> Training the NN takes 0.0307 sec on average across 34905 steps 
  |--> LOCAL AVERAGE REWARD 9.0703125, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 9.0703125, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 10.175514221191406, CRITIC LOSS 0.08644833415746689
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:01,621 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 35500
     |--> Training the NN takes 0.0304 sec on average across 35405 steps 
  |--> LOCAL AVERAGE REWARD 8.796875, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.796875, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 9.722871780395508, CRITIC LOSS 0.07116815447807312
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:11,169 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 36000
     |--> Training the NN takes 0.0302 sec on average across 35905 steps 
  |--> LOCAL AVERAGE REWARD 8.859375, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.859375, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 10.198617935180664, CRITIC LOSS 0.07085474580526352
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:19,195 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 36500
     |--> Training the NN takes 0.0299 sec on average across 36405 steps 
  |--> LOCAL AVERAGE REWARD 8.8671875, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.8671875, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 10.233083724975586, CRITIC LOSS 0.07052931189537048
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:27,238 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 37000
     |--> Training the NN takes 0.0296 sec on average across 36905 steps 
  |--> LOCAL AVERAGE REWARD 8.7734375, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.7734375, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 10.59445858001709, CRITIC LOSS 0.07315605878829956
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:35,436 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 37500
     |--> Training the NN takes 0.0294 sec on average across 37405 steps 
  |--> LOCAL AVERAGE REWARD 8.5703125, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.5703125, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 9.774566650390625, CRITIC LOSS 0.051592420786619186
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:42,627 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 38000
     |--> Training the NN takes 0.0291 sec on average across 37905 steps 
  |--> LOCAL AVERAGE REWARD 8.3515625, MAX INSTANT REWARD REACHED 11.937425138201768
  |--> LOCAL AVERAGE BASIC REWARD 8.3515625, MAXIMUM INSTANT BASIC REWARD: 11.9374
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(11.94), 'Downlink reward': np.float16(5.63), 'Uplink reward': np.float16(6.67), 'Eavesdropping reward': np.float16(0.3613), 'Eavesdropping_Downlink_reward': np.float16(0.357), 'Eavesdropping_Uplink_reward': np.float16(0.1912)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(2.486e-05), 'Uplink reward': np.float16(0.0005555), 'Eavesdropping reward': np.float16(1.308), 'Eavesdropping_Downlink_reward': np.float16(0.0002825), 'Eavesdropping_Uplink_reward': np.float16(1.308)}}
 |--> ACTOR LOSS 9.858245849609375, CRITIC LOSS 0.07831033319234848
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:50,076 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 38500
     |--> Training the NN takes 0.0289 sec on average across 38405 steps 
  |--> LOCAL AVERAGE REWARD 8.59375, MAX INSTANT REWARD REACHED 12.0315320702636
  |--> LOCAL AVERAGE BASIC REWARD 8.59375, MAXIMUM INSTANT BASIC REWARD: 12.0315
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(12.03), 'Downlink reward': np.float16(5.805), 'Uplink reward': np.float16(6.605), 'Eavesdropping reward': np.float16(0.3777), 'Eavesdropping_Downlink_reward': np.float16(0.3455), 'Eavesdropping_Uplink_reward': np.float16(0.03635)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(1.88e-05), 'Uplink reward': np.float16(0.000674), 'Eavesdropping reward': np.float16(1.729), 'Eavesdropping_Downlink_reward': np.float16(0.0001543), 'Eavesdropping_Uplink_reward': np.float16(1.729)}}
 |--> ACTOR LOSS 9.409280776977539, CRITIC LOSS 0.05816793441772461
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:35:57,222 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 39000
     |--> Training the NN takes 0.0286 sec on average across 38905 steps 
  |--> LOCAL AVERAGE REWARD 8.3046875, MAX INSTANT REWARD REACHED 12.0315320702636
  |--> LOCAL AVERAGE BASIC REWARD 8.3046875, MAXIMUM INSTANT BASIC REWARD: 12.0315
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(12.03), 'Downlink reward': np.float16(5.805), 'Uplink reward': np.float16(6.605), 'Eavesdropping reward': np.float16(0.3777), 'Eavesdropping_Downlink_reward': np.float16(0.3455), 'Eavesdropping_Uplink_reward': np.float16(0.03635)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(1.88e-05), 'Uplink reward': np.float16(0.000674), 'Eavesdropping reward': np.float16(1.729), 'Eavesdropping_Downlink_reward': np.float16(0.0001543), 'Eavesdropping_Uplink_reward': np.float16(1.729)}}
 |--> ACTOR LOSS 9.412272453308105, CRITIC LOSS 0.06472882628440857
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:36:05,073 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 39500
     |--> Training the NN takes 0.0284 sec on average across 39405 steps 
  |--> LOCAL AVERAGE REWARD 8.3515625, MAX INSTANT REWARD REACHED 12.370158181491298
  |--> LOCAL AVERAGE BASIC REWARD 8.3515625, MAXIMUM INSTANT BASIC REWARD: 12.3702
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(12.37), 'Downlink reward': np.float16(6.21), 'Uplink reward': np.float16(6.492), 'Eavesdropping reward': np.float16(0.331), 'Eavesdropping_Downlink_reward': np.float16(0.3027), 'Eavesdropping_Uplink_reward': np.float16(0.02826)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0), 'Uplink reward': np.float16(0.002401), 'Eavesdropping reward': np.float16(2.08), 'Eavesdropping_Downlink_reward': np.float16(6e-08), 'Eavesdropping_Uplink_reward': np.float16(2.08)}}
 |--> ACTOR LOSS 9.153276443481445, CRITIC LOSS 0.0813421681523323
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

2025-09-28 13:36:11,931 - TrainingLogger - VERBOSE - 
|--> TRAINING EPISODE 0, STEP 40000
     |--> Training the NN takes 0.0281 sec on average across 39905 steps 
  |--> LOCAL AVERAGE REWARD 8.96875, MAX INSTANT REWARD REACHED 12.370158181491298
  |--> LOCAL AVERAGE BASIC REWARD 8.96875, MAXIMUM INSTANT BASIC REWARD: 12.3702
  |--> Detailed basic reward for best case: {0: {'Final reward': np.float16(12.37), 'Downlink reward': np.float16(6.21), 'Uplink reward': np.float16(6.492), 'Eavesdropping reward': np.float16(0.331), 'Eavesdropping_Downlink_reward': np.float16(0.3027), 'Eavesdropping_Uplink_reward': np.float16(0.02826)}, 1: {'Final reward': np.float16(0.0), 'Downlink reward': np.float16(0.0), 'Uplink reward': np.float16(0.002401), 'Eavesdropping reward': np.float16(2.08), 'Eavesdropping_Downlink_reward': np.float16(6e-08), 'Eavesdropping_Uplink_reward': np.float16(2.08)}}
 |--> ACTOR LOSS 10.255313873291016, CRITIC LOSS 0.06081850454211235
 |--> USER FAIRNESS FOR THE BEST INSTANT REWARD 0.5, LOCAL USER FAIRNESS 0.5
 |--> POWER DEPLOYED: 0.10000000149011612 Watts
---------------------------

