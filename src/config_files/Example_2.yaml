# config.yaml
# config.yaml

Environment:
  BS_max_power: 20 # Maximum transmit power in dBm authorized for the Base Station
  num_BS_antennas: [4, 4] # Number of transmitting and receiving antennas at the Base Station
  RIS_position: [20, 100] # position of the RIS
  num_RIS_elements: 36 # Number of cells in the RIS
  num_users: 2 # Number of legitimate users
  user_transmit_power: 100 # Maximum transmit power in mW authorized for the Users (Uplink), 0 for disabling uplink comms
  users_position_changing: false # determine wether or not the position of users change at each episode, including episode 0
  user_spawn_limits: [[120, 180], [20, 80]] # Area in which users can take their positions at each episode
  users_fixed_positions: # specified positions in case users_position_changing = True
    - [147, 20]  
    - [169, 80]
  num_eavesdroppers: 2 # Number of eavesdroppers
  eaves_position_changing: false # determine wether or not the position of eavesdroppers change at each episode, including episode 0
  eaves_fixed_positions:
    - [140, 40]  
    - [150, 75] 
  env_seed: 84 # random seed for all the randomness involved in the environment
  eval_env_seed: 42
  verbose: True
  lambda_h: 0.1 # wavelength in meter
  d_h: 0.05 # interspacing between two ULA cells (RIS and BS)
  los_only: true # wether or not to use NLoS channel or only LoS channels
  rician_factor: 10 # Rician factor for the channel modeling
  channel_bandwidth: 10
  noise_power_density: -174
  SI_coefficient: 1
  HWI_coefficients: [0.01, 0.01, 0.01, 0.01]
  decisive_reward_functions: ['baseline_reward'] # possible reward functions :'baseline_reward', 'qos_reward', 'minmax_reward', 'minmax_smoothed_reward'
  informative_reward_functions: []
  target_date_rate: 2.5 # target date rate criteria in bps/hz for qos_reward
  target_secrecy_rate: 1.5 # target_secrecy_rate criteria in bps/hz for qos_reward
  p_f: 2
  user_seed: 84 #seed used specifically for the the placement of the users in case of random locations

Network:
  algorithm: ddpg
  actor_linear_layers: [1024, 512, 256, 128]
  critic_linear_layers: [128, 128, 128]
  gamma: 0.1
  optimizer: adam
  actor_lr: 0.0001
  critic_lr: 0.001
  tau: 0.0005
  critic_tau: 0.001
  actor_frequency_update: 2
  critic_frequency_update: 1


Training_parameters:
  number_episodes: 1
  max_steps_per_episode: 40000
  buffer_size: 5000
  batch_size: 96
  frequency_information: 500
  network_save_checkpoint_frequency: 500
  n_rollout_threads: 1
  n_training_threads: 1
  conduct_evaluation: False
  batch_instead_of_buff: true
  n_eval_rollout_threads: 1
  episode_per_eval_env: 1
  rendering: True