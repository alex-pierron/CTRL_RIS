# config.yaml

Environment:
  num_BS_antennas: [4,4] # transmit antennas and receiving antennas
  num_RIS_elements: 36
  num_users: 2
  num_eavesdroppers: 2
  BS_max_power: 20 # in dBm
  user_transmit_power: 100 # in mW
  user_spawn_limits:
    - [120,180] #[100, 110]
    - [20,80] #[65, 75]
  env_seed: 84
  eval_env_seed: 42
  verbose: true
  lambda_h: 0.1
  alpha: 2
  d_h: 0.05
  RIS_position: [20,100]
  rician_factor: 10
  test_point_for_user: [116, 94]
  test_point_for_BS : [130,90]
  channel_bandwidth: 10 # in MHz
  sigma_noise_power_density: -174 # in DBm/Hz
  delta_k_squared: 3.981e-14 # power in Watts
  action_noise_std: 1
  SI_coefficient: 1
  debugging : true
  HWI_coefficients: [0.01, 0.01, 0.01, 0.01]
  epsilon_0: 1.0
  epsilon_min: 0.01
  decay_rate: 0.01
  decay_type: "exponential"
  size: [200, 200]
  mu: 10
  mu_f: 50
  users_position_changing: true
  users_fixed_positions:
    - [125,92]  
    - [163,28] #180, 71  111, 18 171,18
  eaves_fixed_positions:
    - [110, 80]  
    - [150, 45] #180, 71  111, 18 171,18
  eaves_position_changing: true
  decisive_reward_functions: ["baseline_reward"] #["fqos_reward_bis"]
  informative_reward_functions: []
  R_min_k : 1.1
  downlink_capacity_reward_threshold: 2.5
  uplink_capacity_reward_threshold: 0.35
  alpha_fairness: 0.20
  print_info_env: false
  los_only: false
  random_random_seed: true

Network:
  algorithm: ddpg
  actor_linear_layers : [1024,512,256,128] #[256,128,128,128,128] #[2048,1024,512,256,128,96]
  critic_linear_layers :  [128,128,128] #[128,128,128] #[1024,512,256,128,96]
  gamma: 0.9
  optimizer: adam
  actor_lr: 0.0005
  critic_lr: 0.001
  tau: 0.0005
  critic_tau: 0.001 
  actor_frequency_update: 2
  critic_frequency_update: 1

  use_per: false
  per_alpha: 0.6
  per_beta_start:  0.4
  per_beta_frames: 7000
  per_epsilon: 1e-6


Training_parameters:
  action_noise_scale: 0.1 #0.01
  number_episodes: 5
  max_steps_per_episode: 2000
  buffer_size: 4000
  batch_size: 96
  frequency_information: 1000
  network_save_checkpoint_frequency: 1000
  Curriculum_Learning: true
  n_rollout_threads: 1
  n_training_threads: 1
  conduct_evaluation: false
  batch_instead_of_buff: true
  n_eval_rollout_threads: 1 # keep to 1 for the moment when n_rollout_threads: 1 and minimum 2 when n_rollout_threads: 2
  episode_per_eval_env: 1
  rendering: true