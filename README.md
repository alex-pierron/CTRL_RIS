# FAIR_RIS_AI (FRAI): Investigating AI Model Fairness Vulnerabilities in Reinforcement Learning-Driven Reconfigurable Intelligent Surfaces for B5G and 6G Frameworks

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-GPLv3-green.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)

## ğŸ¯ Overview

This repository investigates fairness vulnerabilities in AI-driven Reconfigurable Intelligent Surfaces (RIS) for Beyond 5G (B5G) and 6G communication frameworks. We focus on identifying, analyzing, and mitigating risks inherent in Reinforcement Learning (RL) control systems to ensure robust and fair implementations for next-generation wireless networks.

### ğŸ”¬ Research Focus
- **Fairness Analysis**: Comprehensive evaluation of AI model fairness in RIS control
- **Vulnerability Assessment**: Identification of potential bias and unfairness sources
- **Mitigation Strategies**: Development of robust RL algorithms for fair RIS operation
- **B5G/6G Integration**: Next-generation wireless network optimization

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+** (3.9+ recommended)
- **FFMPEG** (for rendering capabilities) - [Download here](https://www.ffmpeg.org/download.html)
- **CUDA** (optional, for GPU acceleration)

### Environment Setup

Choose your preferred setup method:

#### ğŸ Option 1: Conda (Recommended)

Conda provides isolated environments with seamless dependency management:

```bash
# Navigate to the project directory
cd /path/to/FAIR_RIS_AI

# For GPU users (recommended for training):
conda env create -f requirements/GPU_environment.yml
conda activate fair_ris_ai_env_GPU

# For CPU users (faster installation):
conda env create -f requirements/CPU_environment.yml
conda activate fair_ris_ai_env_CPU
```

#### ğŸ³ Option 2: Docker (Cross-platform)

For a containerized setup that works on both CPU and GPU:

```bash
# Pull the Docker image
docker pull ap496/fair_ris_ai

# Run the Docker container interactively
docker run -it ap496/fair_ris_ai
```

#### ğŸ Option 3: Python venv

For a lightweight virtual environment:

```bash
# Create and activate virtual environment
python -m venv fair_ris_env

# Activate (choose your platform):
# macOS/Linux:
source fair_ris_env/bin/activate

# Windows PowerShell:
.\fair_ris_env\Scripts\Activate.ps1

# Windows CMD:
.\fair_ris_env\Scripts\activate.bat

# Install dependencies
pip install --upgrade pip
pip install -r requirements/requirements_gpu_environment.txt  # GPU users
# OR
pip install -r requirements/requirements_cpu_environment.txt  # CPU users
```

## ğŸ§ª Running Your First Experiments

### Step 1: Choose Your Configuration

The project includes several pre-configured experiment setups in `src/config_files/`:

- **`Example_1.yaml`** - Basic RIS fairness analysis
- **`Example_2.yaml`** - Advanced multi-user scenarios  
- **`Example_3.yaml`** - Complex B5G/6G network simulation
- **`test.yaml`** - Quick testing configuration

### Step 2: Launch Your Experiment

```bash
# Navigate to the training directory
cd src/train

# Run your experiment (replace 'your_config_file' with actual filename)
python train_RIS.py --config_file Example_1.yaml

# For custom configurations:
python train_RIS.py --config_file your_custom_config.yaml
```

### Step 3: Monitor Results

Your experiment will automatically:
- âœ… **Save training logs** in `data/analytics/paper/`
- ğŸ“Š **Generate performance plots** and visualizations
- ğŸ¬ **Create animation files** (if enabled)
- ğŸ“ˆ **Export metrics** for analysis

### ğŸ”§ Customizing Experiments

Create your own configuration by copying an existing YAML file:

```bash
# Copy a template
cp src/config_files/Example_1.yaml src/config_files/my_experiment.yaml

# Edit the configuration
nano src/config_files/my_experiment.yaml  # or use your preferred editor
```

### ğŸ“Š Understanding Output

Results are organized as follows:
```
data/analytics/paper/
â”œâ”€â”€ Example_1/
â”‚   â”œâ”€â”€ ddpg/
â”‚   â”‚   â”œâ”€â”€ Baseline/          # Baseline algorithm results
â”‚   â”‚   â”œâ”€â”€ R_fm/             # Fairness metric rewards
â”‚   â”‚   â”œâ”€â”€ R_fs/             # Fairness smoothed rewards  
â”‚   â”‚   â””â”€â”€ R_qos/            # QoS-based rewards
â”‚   â””â”€â”€ plots/                # Generated visualizations
â””â”€â”€ ...
```

### ğŸš€ Advanced Usage

For more complex experiments, you can:
- **Modify algorithm parameters** in the YAML files
- **Adjust environment settings** for different scenarios
- **Enable/disable rendering** for faster training
- **Configure multi-processing** for parallel execution
## ğŸ“ Repository Structure

```
FAIR_RIS_AI/
â”œâ”€â”€ ğŸ“Š data/                          # Experimental data and results
â”‚   â”œâ”€â”€ analytics/                    # Analysis results and visualizations
â”‚   â”œâ”€â”€ archived_experiments/         # Historical experiment data
â”‚   â””â”€â”€ raw/                          # Raw experimental outputs
â”œâ”€â”€ ğŸ’» src/                           # Source code
â”‚   â”œâ”€â”€ ğŸ§  algorithms/               # Reinforcement Learning algorithms
â”‚   â”‚   â”œâ”€â”€ ddpg.py                  # Deep Deterministic Policy Gradient
â”‚   â”‚   â”œâ”€â”€ td3.py                   # Twin Delayed DDPG
â”‚   â”‚   â”œâ”€â”€ sac.py                   # Soft Actor-Critic
â”‚   â”‚   â”œâ”€â”€ ppo.py                   # Proximal Policy Optimization
â”‚   â”‚   â””â”€â”€ replay_buffer.py         # Experience replay buffer
â”‚   â”œâ”€â”€ âš™ï¸ config_files/             # Experiment configurations
â”‚   â”œâ”€â”€ ğŸŒ environment/              # RL environment implementation
â”‚   â”‚   â”œâ”€â”€ RIS_duplex.py           # Main RIS environment
â”‚   â”‚   â”œâ”€â”€ physics/                 # Signal processing and channel modeling
â”‚   â”‚   â”œâ”€â”€ rewards/                 # Reward function implementations
â”‚   â”‚   â””â”€â”€ multiprocessing/         # Parallel execution support
â”‚   â”œâ”€â”€ ğŸƒ runners/                  # Training execution scripts
â”‚   â””â”€â”€ ğŸ¯ train/                    # Main training and evaluation scripts
â”œâ”€â”€ ğŸ³ Dockerfile                    # Container configuration
â”œâ”€â”€ ğŸ“‹ requirements/                 # Environment specifications
â””â”€â”€ ğŸ“– README.md                     # This documentation
```

## ğŸ” Key Components

### ğŸ§  Reinforcement Learning Algorithms

| Algorithm | File | Description | Use Case |
|-----------|------|-------------|----------|
| **DDPG** | `ddpg.py` | Deep Deterministic Policy Gradient for continuous control | Baseline RIS control |
| **TD3** | `td3.py` | Twin Delayed DDPG with improved stability | Enhanced performance |
| **SAC** | `sac.py` | Soft Actor-Critic for sample efficiency | State-of-the-art comparison |
| **PPO** | `ppo.py` | Proximal Policy Optimization | On-policy alternative |
| **Buffer** | `replay_buffer.py` | Experience replay management | All off-policy algorithms |

### ğŸŒ RIS Environment

- **`RIS_duplex.py`**: Core Gymnasium-based RL environment
  - Full-duplex communication system modeling
  - Base Station (BS) and user interactions
  - RIS-assisted signal quality optimization
  - Multi-user fairness evaluation

- **`physics/signal_processing.py`**: Physics-based simulation
  - Wireless channel modeling
  - Signal propagation calculations
  - Duplex communication processing
  - Realistic B5G/6G network conditions

### âš™ï¸ Configuration System

- **YAML-based configuration** for all experiment parameters
- **Modular design** allowing easy parameter modification
- **Pre-configured examples** for different scenarios
- **Validation** of configuration parameters

### ğŸƒ Training Infrastructure

- **Multi-processing support** for parallel execution
- **GPU acceleration** for neural network training
- **Real-time monitoring** and logging
- **Automated result visualization**

## ğŸ¤ Contributing

We welcome contributions from the community to enhance FAIR_RIS_AI's capabilities and robustness. Your contributions help advance research in AI fairness for next-generation wireless networks.

### How to Contribute

1. **ğŸ› Bug Reports**: Found an issue? Please report it with detailed reproduction steps
2. **ğŸ“š Documentation**: Help improve our documentation and examples
3. **ğŸ”¬ Research**: Submit experimental results and analysis
4. **ğŸ’¡ Features**: Propose new algorithms or environment enhancements
5. **ğŸ§ª Testing**: Help improve test coverage and validation

### Development Workflow

```bash
# Fork the repository
git clone https://github.com/your-username/FAIR_RIS_AI.git
cd FAIR_RIS_AI

# Create a feature branch
git checkout -b feature/your-feature-name

# Make your changes and test
python -m pytest tests/

# Submit a pull request
git push origin feature/your-feature-name
```

## ğŸ“š Citation

If you use FAIR_RIS_AI in your research, please cite our work:

```bibtex
@misc{FAIR_RIS_AI_2025,
  author = {Pierron, Barbeau and De Cicco, Rubio-Hernan and Garcia-Alfaro},
  title = {FAIR_RIS_AI: Investigating AI Model Fairness Vulnerabilities in Reinforcement Learning-Driven Reconfigurable Intelligent Surfaces for B5G and 6G Frameworks},
  year = {2025},
  howpublished = {\url{https://github.com/alex-pierron/FAIR_RIS_AI}},
  note = {Available at: \url{https://github.com/alex-pierron/FAIR_RIS_AI}}
}
```

## ğŸ“„ License

This project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.


<div align="center">

**â­ Star this repository if you find it useful!**

[![GitHub stars](https://img.shields.io/github/stars/alex-pierron/FAIR_RIS_AI?style=social)](https://github.com/alex-pierron/FAIR_RIS_AI)
[![GitHub forks](https://img.shields.io/github/forks/alex-pierron/FAIR_RIS_AI?style=social)](https://github.com/alex-pierron/FAIR_RIS_AI)

</div>